# Metrics Explanation

This document explains the evaluation metrics used in the deepfake detection pipeline.

---

## ðŸ“Š Confusion Matrix

The confusion matrix shows actual vs predicted labels:

```
                 Predicted
              Real    Fake
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
Actual   â”‚   TN   â”‚   FP   â”‚   Real
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚   FN   â”‚   TP   â”‚   Fake
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Term | Meaning | For Deepfake Detection |
|------|---------|------------------------|
| **TN** | Real â†’ predicted Real | âœ… Correct |
| **FP** | Real â†’ predicted Fake | âš ï¸ False alarm |
| **FN** | Fake â†’ predicted Real | âŒ **Dangerous** â€” deepfake spreads |
| **TP** | Fake â†’ predicted Fake | âœ… Correct |

---

## ðŸ“ˆ Key Metrics

### Accuracy
```
Accuracy = (TP + TN) / Total
```
- **Problem:** Misleading with imbalanced data
- **Example:** 80% fake â†’ always predict fake = 80% accuracy

### Precision
```
Precision = TP / (TP + FP)
```
- **Meaning:** "Of all flagged videos, how many were actually fake?"
- **High precision:** Few false alarms

### Recall (Sensitivity)
```
Recall = TP / (TP + FN)
```
- **Meaning:** "Of all fake videos, how many did I catch?"
- **High recall:** Catches most fakes

### F1 Score
```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```
- **Meaning:** Harmonic mean of precision and recall
- **Why F1?** Balances catching fakes vs avoiding false alarms

### AUC (Area Under ROC Curve)
- **Meaning:** Probability that a random fake scores higher than a random real
- **Range:** 0.5 (random) to 1.0 (perfect)

---

## âš ï¸ Why False Negatives Matter

In deepfake detection, **False Negatives are the dangerous error**:

| Error | Consequence |
|-------|-------------|
| FP (False Alarm) | User reviews video, realizes it's real. Minor inconvenience. |
| FN (Missed Fake) | **Deepfake spreads undetected.** Can cause real harm. |

---

## ðŸŽ¯ Why I Use F1 Score

| Metric | Problem |
|--------|---------|
| Accuracy | Can be gamed with class imbalance |
| Pure Recall | Would flag everything as fake |
| Pure Precision | Would miss subtle fakes |
| **F1 Score** | âœ… Balances precision and recall |

I use F1 score for:
- Model selection (best model = highest F1)
- Early stopping (stop when F1 plateaus)

---

## ðŸ“‹ My Results

| Metric | Value | Interpretation |
|--------|-------|----------------|
| Accuracy | 100% | All predictions correct |
| Precision | 1.00 | No false alarms |
| Recall | 1.00 | All fakes caught |
| F1 Score | 1.00 | Perfect balance |
| AUC | 1.00 | Perfect separation |

**Note:** Perfect scores on 50-video dataset. Real-world generalization requires larger datasets.

---

## ðŸ”§ Threshold Selection

Default threshold is 0.5:
- score > 0.5 â†’ Fake
- score â‰¤ 0.5 â†’ Real

Can be adjusted for different use cases:

| Threshold | Effect |
|-----------|--------|
| 0.3 | Conservative â€” catch more fakes, more false alarms |
| 0.5 | Balanced (default) |
| 0.7 | Strict â€” fewer false alarms, might miss subtle fakes |
